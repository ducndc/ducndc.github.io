---
layout: post
title: "Out-of-Memory Management in the Linux Kernel"
date: 2025-10-05 10:00:00 +0700
categories: [Linux]
tags: [linux, kernel, memory, oom, mm]
excerpt: "A deep dive into how the Linux OOM killer works — from checking available memory and determining OOM status, to selecting and killing a victim process."
---

When a Linux system runs critically low on memory, it doesn't just crash — it invokes the **OOM killer**, a subsystem with one job: verify the system is genuinely out of memory and, if so, select a process to kill in order to free enough memory for the system to continue.

This post walks through the full flow: how the kernel checks available memory, how it decides OOM has truly occurred, how it selects a victim, and what the relevant kernel source looks like at each step.

## Table of Contents

1. [Checking Available Memory](#checking-available-memory)
2. [Determining OOM Status](#determining-oom-status)
3. [Selecting a Process to Kill](#selecting-a-process-to-kill)
4. [Full Call Flow](#full-call-flow)
5. [Kernel Source Walkthrough](#kernel-source-walkthrough)

---

## Checking Available Memory

Before the system reaches a true OOM condition, the kernel tries to avoid it. Certain operations — like expanding the heap with `brk()` or remapping an address space with `mremap()` — trigger an upfront check to see if enough memory exists to satisfy the request.

This check is performed by `vm_enough_memory()`, which receives the number of required pages as a parameter. Unless the administrator has configured the system to overcommit memory, the function tallies the following reclaimable and available resources:

| Source | Reason Counted |
|--------|----------------|
| Total page cache pages | Easily reclaimed under pressure |
| Total free pages | Already available immediately |
| Total free swap pages | Userspace pages can be paged out |
| Pages managed by `swapper_space` | Slightly double-counts free swap, but balances against reserved-but-unused slots |
| Dentry cache pages | Easily reclaimed |
| Inode cache pages | Easily reclaimed |

If the sum is sufficient for the request, `vm_enough_memory()` returns `true`. Otherwise it returns `false`, and the caller typically returns `-ENOMEM` to userspace — no page scanning or process killing required.

---

## Determining OOM Status

If the page reclaim path runs at its highest priority and still cannot free enough pages to satisfy an allocation, `out_of_memory()` is called. But the kernel is deliberately conservative here: just because reclaim failed doesn't mean the system is truly OOM. It may simply need to wait for I/O to complete or for pages to finish swapping out.

To avoid killing processes unnecessarily, the kernel runs through the following checklist before invoking the OOM killer:

| Condition | Interpretation |
|-----------|----------------|
| Swap space still available (`nr_swap_pages > 0`) | Not OOM — pages can still be swapped out |
| Last failure was more than 5 seconds ago | Not OOM — previous pressure may have resolved |
| No failure in the last second | Not OOM — too early to be certain |
| Fewer than 10 failures in the last 5 seconds | Not OOM — not enough sustained pressure |
| A process was already killed in the last 5 seconds | Not OOM — wait for the kill to take effect |

Only if all five checks are passed does the kernel proceed to `oom_kill()`.

![OOM decision flow](/assets/img/linux/oom.png)

---

## Selecting a Process to Kill

`select_bad_process()` iterates over all running tasks and scores each one using `oom_badness()`. The process with the highest score becomes the victim.

### The Badness Heuristic

The modern `oom_badness()` scores a task based on its memory footprint relative to total available RAM:

```
points = RSS + swap pages + page table size (in pages)
points += oom_score_adj * (totalpages / 1000)
```

The `oom_score_adj` value (ranging from -1000 to +1000) is a per-process tunable that lets applications and administrators influence the OOM killer's decisions:

- `-1000` makes a process completely immune to OOM killing
- `+1000` makes it the first target
- Root processes and those with `CAP_SYS_ADMIN` or `CAP_SYS_RAWIO` are generally deprioritized

Processes are also skipped entirely if they are already exiting, are kernel threads, are in the middle of a `vfork()`, or have been marked with `MMF_OOM_SKIP`.

### What Happens After Selection

Once a victim is chosen, `oom_kill_process()` sends `SIGKILL` to the process. If the process is already exiting, the killer simply gives it access to memory reserves to let it exit quickly. If the victim belongs to a memory cgroup configured with group killing, all tasks in that cgroup are killed together.

---

## Full Call Flow

```
User process calls malloc() / new / mmap() / fork()
    ↓
__alloc_pages()
    ↓
__alloc_pages_nodemask()        ← fast path: try freelist directly
    ↓ (fast path fails)
__alloc_pages_slowpath()        ← wake kswapd, try compaction, direct reclaim
    ↓ (all reclaim exhausted)
__alloc_pages_may_oom()         ← acquire oom_lock, one last freelist attempt
    ↓
out_of_memory()                 ← run OOM checklist, notify listeners
    ↓
select_bad_process()            ← iterate all tasks
    ↓
oom_evaluate_task()             ← score each candidate
    ↓
oom_badness()                   ← compute RSS + swap + pagetable score
    ↓
oom_kill_process()              ← send SIGKILL to chosen victim
    ↓
Memory freed → system continues
```

---

## Kernel Source Walkthrough

### `__alloc_pages()` and `__alloc_pages_nodemask()`

The entry point for all page allocations. Tries the freelist directly on the fast path; falls through to the slow path if that fails.

```c
static inline struct page *
__alloc_pages(gfp_t gfp_mask, unsigned int order, int preferred_nid)
{
    return __alloc_pages_nodemask(gfp_mask, order, preferred_nid, NULL);
}
```

```c
struct page *
__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
                       nodemask_t *nodemask)
{
    struct page *page;
    unsigned int alloc_flags = ALLOC_WMARK_LOW;
    gfp_t alloc_mask;
    struct alloc_context ac = { };

    if (unlikely(order >= MAX_ORDER)) {
        WARN_ON_ONCE(!(gfp_mask & __GFP_NOWARN));
        return NULL;
    }

    gfp_mask &= gfp_allowed_mask;
    alloc_mask = gfp_mask;
    if (!prepare_alloc_pages(gfp_mask, order, preferred_nid, nodemask,
                             &ac, &alloc_mask, &alloc_flags))
        return NULL;

    alloc_flags |= alloc_flags_nofragment(ac.preferred_zoneref->zone, gfp_mask);

    /* Fast path: try the freelist */
    page = get_page_from_freelist(alloc_mask, order, alloc_flags, &ac);
    if (likely(page))
        goto out;

    alloc_mask = current_gfp_context(gfp_mask);
    ac.spread_dirty_pages = false;
    ac.nodemask = nodemask;

    /* Slow path: reclaim, compact, and potentially OOM */
    page = __alloc_pages_slowpath(alloc_mask, order, &ac);

out:
    if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
        unlikely(__memcg_kmem_charge_page(page, gfp_mask, order) != 0)) {
        __free_pages(page, order);
        page = NULL;
    }

    trace_mm_page_alloc(page, order, alloc_mask, ac.migratetype);
    return page;
}
EXPORT_SYMBOL(__alloc_pages_nodemask);
```

---

### `__alloc_pages_slowpath()`

The slow path tries progressively more aggressive strategies: waking `kswapd`, direct compaction, direct reclaim, and finally OOM killing. The `__GFP_NOFAIL` flag causes it to loop indefinitely rather than fail.

```c
static inline struct page *
__alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
                       struct alloc_context *ac)
{
    bool can_direct_reclaim = gfp_mask & __GFP_DIRECT_RECLAIM;
    const bool costly_order = order > PAGE_ALLOC_COSTLY_ORDER;
    struct page *page = NULL;
    unsigned int alloc_flags;
    unsigned long did_some_progress;
    enum compact_priority compact_priority;
    enum compact_result compact_result;
    int compaction_retries;
    int no_progress_loops;
    unsigned int cpuset_mems_cookie;
    int reserve_flags;

    if (WARN_ON_ONCE((gfp_mask & (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)) ==
                     (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))
        gfp_mask &= ~__GFP_ATOMIC;

retry_cpuset:
    compaction_retries = 0;
    no_progress_loops = 0;
    compact_priority = DEF_COMPACT_PRIORITY;
    cpuset_mems_cookie = read_mems_allowed_begin();

    alloc_flags = gfp_to_alloc_flags(gfp_mask);
    ac->preferred_zoneref = first_zones_zonelist(ac->zonelist,
                    ac->highest_zoneidx, ac->nodemask);
    if (!ac->preferred_zoneref->zone)
        goto nopage;

    if (alloc_flags & ALLOC_KSWAPD)
        wake_all_kswapds(order, gfp_mask, ac);

    page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
    if (page)
        goto got_pg;

    /* For costly or non-movable high-order allocs, try compaction first */
    if (can_direct_reclaim &&
            (costly_order ||
               (order > 0 && ac->migratetype != MIGRATE_MOVABLE))
            && !gfp_pfmemalloc_allowed(gfp_mask)) {
        page = __alloc_pages_direct_compact(gfp_mask, order,
                        alloc_flags, ac, INIT_COMPACT_PRIORITY,
                        &compact_result);
        if (page)
            goto got_pg;

        if (costly_order && (gfp_mask & __GFP_NORETRY)) {
            if (compact_result == COMPACT_SKIPPED ||
                compact_result == COMPACT_DEFERRED)
                goto nopage;
            compact_priority = INIT_COMPACT_PRIORITY;
        }
    }

retry:
    if (alloc_flags & ALLOC_KSWAPD)
        wake_all_kswapds(order, gfp_mask, ac);

    reserve_flags = __gfp_pfmemalloc_flags(gfp_mask);
    if (reserve_flags)
        alloc_flags = current_alloc_flags(gfp_mask, reserve_flags);

    if (!(alloc_flags & ALLOC_CPUSET) || reserve_flags) {
        ac->nodemask = NULL;
        ac->preferred_zoneref = first_zones_zonelist(ac->zonelist,
                    ac->highest_zoneidx, ac->nodemask);
    }

    page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
    if (page)
        goto got_pg;

    if (!can_direct_reclaim)
        goto nopage;

    if (current->flags & PF_MEMALLOC)
        goto nopage;

    page = __alloc_pages_direct_reclaim(gfp_mask, order, alloc_flags, ac,
                                        &did_some_progress);
    if (page)
        goto got_pg;

    page = __alloc_pages_direct_compact(gfp_mask, order, alloc_flags, ac,
                    compact_priority, &compact_result);
    if (page)
        goto got_pg;

    if (gfp_mask & __GFP_NORETRY)
        goto nopage;

    if (costly_order && !(gfp_mask & __GFP_RETRY_MAYFAIL))
        goto nopage;

    if (should_reclaim_retry(gfp_mask, order, ac, alloc_flags,
                 did_some_progress > 0, &no_progress_loops))
        goto retry;

    if (did_some_progress > 0 &&
            should_compact_retry(ac, order, alloc_flags,
                compact_result, &compact_priority, &compaction_retries))
        goto retry;

    if (check_retry_cpuset(cpuset_mems_cookie, ac))
        goto retry_cpuset;

    /* All reclaim exhausted — invoke OOM killer */
    page = __alloc_pages_may_oom(gfp_mask, order, ac, &did_some_progress);
    if (page)
        goto got_pg;

    if (tsk_is_oom_victim(current) &&
        (alloc_flags & ALLOC_OOM || (gfp_mask & __GFP_NOMEMALLOC)))
        goto nopage;

    if (did_some_progress) {
        no_progress_loops = 0;
        goto retry;
    }

nopage:
    if (check_retry_cpuset(cpuset_mems_cookie, ac))
        goto retry_cpuset;

    if (gfp_mask & __GFP_NOFAIL) {
        if (WARN_ON_ONCE(!can_direct_reclaim))
            goto fail;
        WARN_ON_ONCE(current->flags & PF_MEMALLOC);
        WARN_ON_ONCE(order > PAGE_ALLOC_COSTLY_ORDER);

        page = __alloc_pages_cpuset_fallback(gfp_mask, order, ALLOC_HARDER, ac);
        if (page)
            goto got_pg;

        cond_resched();
        goto retry;
    }
fail:
    warn_alloc(gfp_mask, ac->nodemask,
               "page allocation failure: order:%u", order);
got_pg:
    return page;
}
```

---

### `__alloc_pages_may_oom()`

Acquires the global `oom_lock` (ensuring only one OOM kill proceeds at a time), makes one final freelist attempt under high watermarks, then calls `out_of_memory()`.

```c
static inline struct page *
__alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,
    const struct alloc_context *ac, unsigned long *did_some_progress)
{
    struct oom_control oc = {
        .zonelist = ac->zonelist,
        .nodemask = ac->nodemask,
        .memcg = NULL,
        .gfp_mask = gfp_mask,
        .order = order,
    };
    struct page *page;

    *did_some_progress = 0;

    if (!mutex_trylock(&oom_lock)) {
        /* Another thread is handling OOM — give it a moment */
        *did_some_progress = 1;
        schedule_timeout_uninterruptible(1);
        return NULL;
    }

    /* One final attempt under high watermark before killing anything */
    page = get_page_from_freelist((gfp_mask | __GFP_HARDWALL) &
                  ~__GFP_DIRECT_RECLAIM, order,
                  ALLOC_WMARK_HIGH|ALLOC_CPUSET, ac);
    if (page)
        goto out;

    /* Skip OOM killer in specific circumstances */
    if (current->flags & PF_DUMPCORE)      goto out;  /* coredump in progress */
    if (order > PAGE_ALLOC_COSTLY_ORDER)   goto out;  /* OOM can't help high-order */
    if (gfp_mask & (__GFP_RETRY_MAYFAIL | __GFP_THISNODE)) goto out;
    if (ac->highest_zoneidx < ZONE_NORMAL) goto out;  /* lowmem alloc */
    if (pm_suspended_storage())            goto out;

    if (out_of_memory(&oc) || WARN_ON_ONCE(gfp_mask & __GFP_NOFAIL)) {
        *did_some_progress = 1;
        if (gfp_mask & __GFP_NOFAIL)
            page = __alloc_pages_cpuset_fallback(gfp_mask, order,
                    ALLOC_NO_WATERMARKS, ac);
    }
out:
    mutex_unlock(&oom_lock);
    return page;
}
```

---

### `out_of_memory()`

Runs the OOM checklist, notifies registered listeners, and dispatches to `select_bad_process()`. If no killable process is found, the kernel panics.

```c
bool out_of_memory(struct oom_control *oc)
{
    unsigned long freed = 0;

    if (oom_killer_disabled)
        return false;

    if (!is_memcg_oom(oc)) {
        blocking_notifier_call_chain(&oom_notify_list, 0, &freed);
        if (freed > 0)
            return true;  /* Notifier freed memory — crisis averted */
    }

    /* If current task is already dying, let it exit quickly */
    if (task_will_free_mem(current)) {
        mark_oom_victim(current);
        wake_oom_reaper(current);
        return true;
    }

    if (oc->gfp_mask && !(oc->gfp_mask & __GFP_FS) && !is_memcg_oom(oc))
        return true;

    oc->constraint = constrained_alloc(oc);
    if (oc->constraint != CONSTRAINT_MEMORY_POLICY)
        oc->nodemask = NULL;
    check_panic_on_oom(oc);

    /* If sysctl_oom_kill_allocating_task is set, kill the requesting task */
    if (!is_memcg_oom(oc) && sysctl_oom_kill_allocating_task &&
        current->mm && !oom_unkillable_task(current) &&
        oom_cpuset_eligible(current, oc) &&
        current->signal->oom_score_adj != OOM_SCORE_ADJ_MIN) {
        get_task_struct(current);
        oc->chosen = current;
        oom_kill_process(oc, "Out of memory (oom_kill_allocating_task)");
        return true;
    }

    select_bad_process(oc);

    if (!oc->chosen) {
        dump_header(oc, NULL);
        pr_warn("Out of memory and no killable processes...\n");
        if (!is_sysrq_oom(oc) && !is_memcg_oom(oc))
            panic("System is deadlocked on memory\n");
    }

    if (oc->chosen && oc->chosen != (void *)-1UL)
        oom_kill_process(oc, !is_memcg_oom(oc) ? "Out of memory" :
                         "Memory cgroup out of memory");
    return !!oc->chosen;
}
```

---

### `select_bad_process()` and `oom_evaluate_task()`

Iterates all tasks and tracks the one with the highest badness score.

```c
static void select_bad_process(struct oom_control *oc)
{
    oc->chosen_points = LONG_MIN;

    if (is_memcg_oom(oc))
        mem_cgroup_scan_tasks(oc->memcg, oom_evaluate_task, oc);
    else {
        struct task_struct *p;
        rcu_read_lock();
        for_each_process(p)
            if (oom_evaluate_task(p, oc))
                break;
        rcu_read_unlock();
    }
}
```

```c
static int oom_evaluate_task(struct task_struct *task, void *arg)
{
    struct oom_control *oc = arg;
    long points;

    if (oom_unkillable_task(task))
        goto next;

    if (!is_memcg_oom(oc) && !oom_cpuset_eligible(task, oc))
        goto next;

    /* Don't disturb an OOM kill already in progress */
    if (!is_sysrq_oom(oc) && tsk_is_oom_victim(task)) {
        if (test_bit(MMF_OOM_SKIP, &task->signal->oom_mm->flags))
            goto next;
        goto abort;
    }

    /* Tasks marked as OOM origins get maximum score */
    if (oom_task_origin(task)) {
        points = LONG_MAX;
        goto select;
    }

    points = oom_badness(task, oc->totalpages);
    if (points == LONG_MIN || points < oc->chosen_points)
        goto next;

select:
    if (oc->chosen)
        put_task_struct(oc->chosen);
    get_task_struct(task);
    oc->chosen = task;
    oc->chosen_points = points;
next:
    return 0;
abort:
    if (oc->chosen)
        put_task_struct(oc->chosen);
    oc->chosen = (void *)-1UL;
    return 1;
}
```

---

### `oom_badness()`

Scores a task based on its total memory footprint, adjusted by its `oom_score_adj` value.

```c
long oom_badness(struct task_struct *p, unsigned long totalpages)
{
    long points;
    long adj;

    if (oom_unkillable_task(p))
        return LONG_MIN;

    p = find_lock_task_mm(p);
    if (!p)
        return LONG_MIN;

    adj = (long)p->signal->oom_score_adj;
    if (adj == OOM_SCORE_ADJ_MIN ||
            test_bit(MMF_OOM_SKIP, &p->mm->flags) ||
            in_vfork(p)) {
        task_unlock(p);
        return LONG_MIN;
    }

    /* Score = RSS + swap usage + page table overhead */
    points = get_mm_rss(p->mm) + get_mm_counter(p->mm, MM_SWAPENTS) +
        mm_pgtables_bytes(p->mm) / PAGE_SIZE;
    task_unlock(p);

    /* Apply oom_score_adj as a normalized offset */
    adj *= totalpages / 1000;
    points += adj;

    return points;
}
```

---

### `oom_kill_process()`

Sends `SIGKILL` to the victim. If the victim belongs to a memory cgroup with group killing configured, all tasks in that cgroup are killed.

```c
static void oom_kill_process(struct oom_control *oc, const char *message)
{
    struct task_struct *victim = oc->chosen;
    struct mem_cgroup *oom_group;
    static DEFINE_RATELIMIT_STATE(oom_rs, DEFAULT_RATELIMIT_INTERVAL,
                                  DEFAULT_RATELIMIT_BURST);

    /* If already exiting, give it memory reserves and let it die */
    task_lock(victim);
    if (task_will_free_mem(victim)) {
        mark_oom_victim(victim);
        wake_oom_reaper(victim);
        task_unlock(victim);
        put_task_struct(victim);
        return;
    }
    task_unlock(victim);

    if (__ratelimit(&oom_rs))
        dump_header(oc, victim);

    oom_group = mem_cgroup_get_oom_group(victim, oc->memcg);

    __oom_kill_process(victim, message);

    /* Kill entire cgroup if configured to do so */
    if (oom_group) {
        mem_cgroup_print_oom_group(oom_group);
        mem_cgroup_scan_tasks(oom_group, oom_kill_memcg_member,
                              (void *)message);
        mem_cgroup_put(oom_group);
    }
}
```

---

## Tuning the OOM Killer

You can influence OOM killer behavior from userspace:

```bash
# Make a process nearly immune to OOM killing
echo -1000 > /proc/<pid>/oom_score_adj

# Make a process the first target
echo 1000 > /proc/<pid>/oom_score_adj

# Disable the OOM killer entirely (risky — system may deadlock instead)
sysctl vm.oom-kill = 0

# Kill the allocating task instead of selecting a victim
sysctl vm.oom_kill_allocating_task = 1

# Allow kernel to overcommit memory (disables vm_enough_memory checks)
sysctl vm.overcommit_memory = 1
```

OOM events are logged to the kernel ring buffer and visible via `dmesg`. A typical OOM log shows the selected victim, its memory usage, and the `oom_score_adj` values of all considered processes.